{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install re\n",
    "!pip install numpy\n",
    "!pip install collections\n",
    "!pip install math\n",
    "\n",
    "!pip install nltk\n",
    "!pip install NRCLex\n",
    "!python -m textblob.download_corpora\n",
    "!pip install beautifulsoup4\n",
    "!pip install tqdm\n",
    "!pip install gensim\n",
    "!pip install sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather the test jokes and extract hand-crafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/')\n",
    "\n",
    "from make_embed import make_embeddings\n",
    "from joke_scraper import scraped_jokes\n",
    "from semantic_features import clean_data, get_pos_tagged_sentences, path_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the syntatic and SSE based features along with the sematic features except 'closest_path', and 'farthest_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 695.84it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 856.41it/s]\n",
      "Processing Sentences: 100%|██████████| 101/101 [00:00<00:00, 703.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting POS Tags for each sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 1474.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting POS Tags Lists each sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 101176.19it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 15312.11it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 50491.62it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'text' : scraped_jokes})\n",
    "\n",
    "emotions_list = ['all']\n",
    "# emotions_list=['surprise', 'trust', 'anticipation']\n",
    "\n",
    "sse_list = ['all']\n",
    "#sse_list=['vp_count', 'avg_VP_len', 'rpnv', 'phrase_length_ratios_NP', 'phrase_length_ratios_VP']\n",
    "\n",
    "#semantic_list = ['all']\n",
    "semantic_list = ['disconnection', 'repitition', 'sense_combination', 'num_alliteration', 'num_rhymes', 'max_alliteration', 'max_rhymes']\n",
    "\n",
    "filtered_emotion_features, filtered_sse_features, filtered_semantic_features= make_embeddings(data, emotions_list=emotions_list, sse_list=sse_list, semantic_list = semantic_list)\n",
    "disconnection, repitition, sense_combination, num_alliteration, num_rhymes, max_alliteration, max_rhymes = filtered_semantic_features[0], filtered_semantic_features[1], filtered_semantic_features[2], filtered_semantic_features[3], filtered_semantic_features[4], filtered_semantic_features[5], filtered_semantic_features[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(filtered_emotion_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 14)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(filtered_sse_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_feature_len = len(filtered_semantic_features)\n",
    "filtered_semantic_features = np.array(filtered_semantic_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the closest_path and the farthest_path features if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0,), (0,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_closest_features, sense_farthest_features = [],  []\n",
    "if semantic_list == ['all'] or 'closest_path' in semantic_list or 'farthest_path' in semantic_list:\n",
    "    words_list = clean_data(data['text'])\n",
    "    pos_sentences = get_pos_tagged_sentences(words_list)\n",
    "    sense_closest_features, sense_farthest_features = path_similarity(pos_sentences)\n",
    "    \n",
    "sense_closest_features, sense_farthest_features = np.array(sense_closest_features), np.array(sense_farthest_features)\n",
    "sense_closest_features.shape, sense_farthest_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concate all the handcrafted features to get the final list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 31)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sense_closest_features and sense_farthest_features:\n",
    "    final_features = np.concatenate((filtered_emotion_features, filtered_sse_features, filtered_semantic_features.reshape(101, sem_feature_len), sense_closest_features.reshape(-1, 1), sense_farthest_features.reshape(-1, 1)), axis=1)\n",
    "elif sense_closest_features:\n",
    "    final_features = np.concatenate((filtered_emotion_features, filtered_sse_features, filtered_semantic_features.reshape(101, sem_feature_len), sense_closest_features.reshape(-1, 1)), axis=1)\n",
    "elif sense_farthest_features:\n",
    "    final_features = np.concatenate((filtered_emotion_features, filtered_sse_features, filtered_semantic_features.reshape(101, sem_feature_len), sense_farthest_features.reshape(-1, 1)), axis=1)\n",
    "else:\n",
    "    final_features = np.concatenate((filtered_emotion_features, filtered_sse_features, filtered_semantic_features.reshape(101, sem_feature_len)), axis=1)\n",
    "\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TODO : Tanisha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
